{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "###########################################################################################\n",
    "    #Proyecto:      Capstone Urosario\n",
    "    #Title:         Georreferenciación y obtención de localidades y UPZ en Bogotá\n",
    "    #Autores:       Juan Manuel Sarmiento y Juan Felipe Gómez\n",
    "    #Fecha:         20 de agosto de 2021\n",
    "    #Última modificación: 23 de septiembre de 2021\n",
    "    #Version:        Python 3.8.3\n",
    "    #Resumen:        Este código genera coordenadas, localidad y upz a partir de ubicaciones geográficas en bases de datos\n",
    "  \t\t\t\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "from office365.sharepoint.files.file import File\n",
    "from office365.runtime.auth.user_credential import UserCredential\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "import googlemaps\n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "from shapely.geometry import shape, Point\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def import_from_Onedrive(site_url, relative_url, user, password):\n",
    "    # Se importa directamente de One Drive la base de datos actualizada\n",
    "    site_url = str(site_url)\n",
    "    relative_url = str(relative_url)\n",
    "    ctx = ClientContext(site_url).with_credentials(UserCredential(str(user), password=str(password)))\n",
    "    response = File.open_binary(ctx, relative_url)\n",
    "    #Se gurada en BytesIO stream\n",
    "    bytes_file_obj = io.BytesIO()\n",
    "    bytes_file_obj.write(response.content)\n",
    "    bytes_file_obj.seek(0) \n",
    "    return bytes_file_obj\n",
    "def create_df():\n",
    "    #Se lee el objeto en formato dataframe. En caso de querer importar la base directamente desde los archivos locales, en la siguiente línea debe reemplazarse bytes_file_obj por el nombre del archivo de la sigueinte forma 'nombre.xlsx' \n",
    "    try:\n",
    "        df = pd.read_excel(bytes_file_obj)\n",
    "    except:\n",
    "        df = pd.read_excel(str(bdd_importar))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoinfo():\n",
    "    if nuevainfo == 'API':\n",
    "        # Utilización de la API de Google Maps para obtener los datos geográfico de cada una de las observaciones\n",
    "        # Hay que tener en cuenta que la API de Google Maps permité una cantidad de requests al mes antes de cobrar por cada request. Si solo se tiene una clave, el uso gratis del código se limita a alrededor de 40.000 requests por mes.\n",
    "        gmaps = googlemaps.Client(key=str(api_key)) #Aquí debe ingresarse la clave individual de la API en formato string\n",
    "        coordenadas = []\n",
    "        for ubicación in list(df.loc[:,\"UBICACIÓN\"]): #se parte de la columna de UBICACIÓN\n",
    "            geocode_result = gmaps.geocode(ubicación, components={\"administrative_area\": \"Bogotá\", \"country\": \"'Colombia'\"})\n",
    "            coordenadas.append(geocode_result)\n",
    "    elif nuevainfo == 'Equipo': \n",
    "        # Si no hay modificaciones ni nuevas obsrvaciones, puede utilizarce la última lista de datos geográficos obtenidos de la API con las siguientes líneas:\n",
    "        # Esto evita desperdiciar los request gratis al mes que permite la API de Google Maps\n",
    "        with open(\"coordenadas.txt\", \"r\") as fp:\n",
    "            coordenadas = json.load(fp)\n",
    "    return coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def emptycorr():\n",
    "    # Para algunas observaciones no es posible obtener datos geográficos, debido a que la variable de unicación está vacía o contiene ubicaciones no comprensibles para Google Maps. Se buscan y visualizan las observaciones de este tipo.\n",
    "    list_emptycoor = []\n",
    "    for i in range(len(coordenadas)):\n",
    "        if coordenadas[i] == []:\n",
    "            list_emptycoor.append(int(i))\n",
    "    print(df.iloc[list_emptycoor]) \n",
    "    # Para facilitar el código, a las observaciones sin datos geográficos se les asiga los datos del PROGRAMA NACIONAL ESCUELAS TALLER DE COLOMBIA. Luego, estas observaciones deben corregirse y buscarse manualmente.\n",
    "    for i in range(len(coordenadas)):\n",
    "        if coordenadas[i] == []:\n",
    "            coordenadas[i] = coordenadas[22]\n",
    "    return coordenadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long():\n",
    "    # De los datos geográficos, se sacan los datos de interés: latitudes y longitudes, para conformar las coordenadas\n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "    for element in coordenadas: # se hace un for loop que bsuca en la estructura de los datos geográficos la información de latitudes y longitudes, luego se agregan a sus respectivas listas en el mismo orden de la base de datos\n",
    "        latitudes.append(element[0]['geometry']['location']['lat'])\n",
    "        longitudes.append(element[0]['geometry']['location']['lng'])\n",
    "    return latitudes, longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_localidades():\n",
    "    # Se genera lista de tuplas con las coordenadas de cada observación\n",
    "    coordinatelist = [list(pair) for pair in zip(longitudes, latitudes)]\n",
    "    # Se genera lista de puntos en el formato necesario (point de la librería Shapely) para comprobar si está dentro de determinado polígono\n",
    "    pointlist = []\n",
    "    for coordinate in coordinatelist:\n",
    "            point = Point(coordinate)\n",
    "            pointlist.append(point)\n",
    "    # Se comprueba dentro de qué polígono de localidades está cada uno de los puntos\n",
    "    aux_localidades = []   # Se hace una lista de lo datos geográficos de los polígonos de las localidades a las que pertenece cada punto, en orden     \n",
    "    aux_foundpoints = []   # se hace una lista de los puntos que pudieron ser encontrados dentro de los polígonos de localidades\n",
    "    with open('poligonos-localidades.geojson') as f: # Se importa el archivo .geojson con los polígonos de las localidades\n",
    "        js = json.load(f)\n",
    "    for point in pointlist: # Se hace la comprobación de a qué polígono pertenece cada punto\n",
    "        for feature in js['features']: \n",
    "            polygon = shape(feature['geometry'])\n",
    "            if point.within(polygon) or point.intersects(polygon): \n",
    "                aux_localidades.append(feature)\n",
    "                aux_foundpoints.append(point)\n",
    "    return pointlist, aux_localidades, aux_foundpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notfound_locpoint():\n",
    "    # Se hace una lista de los puntos que NO fueron econtrados en los polígonos de localidades\n",
    "    point_dictionary = dict(zip(list(range(len(df.loc[:,\"UBICACIÓN\"]))), pointlist))\n",
    "    aux_notfoundpoints = []\n",
    "    for key, value in point_dictionary.items():\n",
    "        if value not in aux_foundpoints:\n",
    "            aux_notfoundpoints.append(key)\n",
    "    return point_dictionary, aux_notfoundpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_num():\n",
    "    # Se extrae de la lista de datos geográficos de las localidades de cada punto la información relevante: nombre y número de localidad\n",
    "    localidades_name = []\n",
    "    localidades_num = []\n",
    "    for element in aux_localidades:\n",
    "        localidades_name.append(element['properties']['Nombre de la localidad'])\n",
    "        localidades_num.append(element['properties']['Identificador unico de la localidad'])\n",
    "    return localidades_name, localidades_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_upz():\n",
    "    # Se compreba dentro de qué polígono de UPZ está cada uno de los puntos\n",
    "    aux_upz = [] # Se hace una lista de lo datos geográficos de los polígonos de las UPZ a las que pertenece cada punto, en orden     \n",
    "    aux_foundupz = [] # se hace una lista de los puntos que pudieron ser encontrados dentro de los polígonos de UPZ\n",
    "    with open('unidad-de-planeamiento13.geojson') as f: # Se importa el archivo .geojson con los polígonos de las UPZ\n",
    "        js = json.load(f)\n",
    "    for point in pointlist: # Se hace la comprobación de a qué polígono pertenece cada punto\n",
    "        for feature in js['features']: \n",
    "            polygon = shape(feature['geometry'])\n",
    "            if point.within(polygon) or point.intersects(polygon): \n",
    "                aux_upz.append(feature)\n",
    "                aux_foundupz.append(point)\n",
    "    return aux_upz, aux_foundupz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notfound_upzpoint():\n",
    "    # Se hace una lista de los puntos que NO fueron econtrados en los polígonos de UPZ\n",
    "    aux_notfoundupz = []\n",
    "    for key, value in point_dictionary.items():\n",
    "        if value not in aux_foundupz:\n",
    "            aux_notfoundupz.append(key)\n",
    "    return aux_notfoundupz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upz_num(): \n",
    "    # Se extrae de la lista de datos geográficos de las UPZ de cada punto la información relevante: nombre y número de UPZ\n",
    "    upz_name = []\n",
    "    upz_num = []\n",
    "    for element in aux_upz:\n",
    "        upz_name.append(element['properties']['uplnombre'])\n",
    "        upz_num.append(element['properties']['uplcodigo'])\n",
    "    return upz_name, upz_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lostloc_dict():\n",
    "    # Se genera lista con el número de cada observación \n",
    "    keys = (list(range(len(df.loc[:,\"UBICACIÓN\"]))))\n",
    "    # Se genera lista de longitud igual a la de puntos no encontrados con el strinh \"not found\" en cada posición\n",
    "    notfountlist = ['not found'] * len(aux_notfoundpoints)\n",
    "    # Se genera una lista de tuplas en la que se combina el número de cada observación con su point, el número de cada observación no encontrada con el string \"not found\" y el número de cada observación encontrada con el número de su localidad \n",
    "    dict_list = list(zip(keys, pointlist)) + list(zip(aux_notfoundpoints, notfountlist)) + list(zip([elem for elem in keys if elem not in aux_notfoundpoints ], localidades_name)) +list(zip([elem for elem in keys if elem not in aux_notfoundpoints ], localidades_num))\n",
    "    # se crea un diccionario a aprtir de la lista de tuplas anterior en el que el número de observación es la key y el value es una lista con el point, el nombre de la localidad y el número de la localidad en caso de encontrarse. Para el caso de los puntos no encontrados, se les asigna el point y \"not found\"\n",
    "    result = {}\n",
    "    for i in dict_list:  \n",
    "       result.setdefault(i[0],[]).append(i[1])\n",
    "    return keys, notfountlist, dict_list, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lostupz_dict():\n",
    "    # Se genera una lista de tuplas en la que se combina el número de cada observación con su point, el número de cada observación no encontrada con el string \"not found\" y el número de cada observación encontrada con el número de su UPZ \n",
    "    notfountlistupz = ['not found'] * len(aux_notfoundupz)\n",
    "    dict_listupz = list(zip(keys, pointlist)) + list(zip(aux_notfoundupz, notfountlistupz)) + list(zip([elem for elem in keys if elem not in aux_notfoundupz ], upz_name)) + list(zip([elem for elem in keys if elem not in aux_notfoundupz ], upz_num))\n",
    "    # se crea un diccionario a aprtir de la lista de tuplas anterior en el que el número de observación es la key y el value es una lista con el point, el nombre de la UPZ y el número de la UPZ en caso de encontrarse. Para el caso de los puntos no encontrados, se les asigna el point y \"not found\"\n",
    "    resultupz = {}\n",
    "    for i in dict_listupz:  \n",
    "       resultupz.setdefault(i[0],[]).append(i[1])\n",
    "    return  notfountlistupz, dict_listupz, resultupz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finaldf():\n",
    "    # Se crea un dataframe con el diccionario anterior\n",
    "    localidades_df = pd.DataFrame.from_dict(result,orient='index', columns = ['Coordenadas', 'Localidad', \n",
    "                                                                              'Número de Localidad'])\n",
    "    # Se crea un dataframe con el diccionario anterior\n",
    "    upz_df = pd.DataFrame.from_dict(resultupz,orient='index', columns = ['Coordenadas', 'UPZ',\n",
    "                                                                     'Número de UPZ'])\n",
    "    # Se crea el data frame final en el que se agrega el nombre de la iniciativa, las coordenadas, localidad, número de localidad, UPZ y número de UPZ\n",
    "    coor_colum = list(zip(latitudes, longitudes))\n",
    "    coor_series = pd.Series(coor_colum)\n",
    "    coor_df = pd.merge(localidades_df, upz_df, how='outer', left_index=True, right_index=True)\n",
    "    del coor_df['Coordenadas_y']\n",
    "    coor_df = coor_df.rename(columns={'Coordenadas_x': 'Coordenadas'})\n",
    "    aux_df = pd.merge(df, coor_df, how='outer', left_index=True, right_index=True)\n",
    "    aux_df = aux_df.rename(columns={'Localidad_y': 'Localidad', 'UPZ_y':'UPZ' })\n",
    "    final_df = aux_df[['NOMBRE DE INICIATIVA', 'Coordenadas', 'Localidad','Número de Localidad', 'UPZ', 'Número de UPZ']]\n",
    "    final_df['COORDEADAS (Latitud, longitud)'] = coor_series\n",
    "    final_df.index += 1\n",
    "    final_df.to_excel(\"Localidades y UPZ2.xlsx\") \n",
    "    return localidades_df, upz_df, final_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_coor():\n",
    "    # Para evitar utilizar la API siempre que se corra el código y malgastar requests gratis, se exporta la lista a un archivo externo\n",
    "    with open(\"coordenadas.txt\", \"w\") as fp:\n",
    "        json.dump(coordenadas, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Importará la base de datos desde el equipo o desde OneDrive?(Equipo/OneDrive)Equipo\n",
      "Ingresar nombre del archivo en formato .xlsx a importarOferta Jovenes GOYN.xlsx\n",
      "¿Desea hacer uso de la API o importar archivo del equipo? (API/Equipo)Equipo\n",
      "5845\n",
      "Empty DataFrame\n",
      "Columns: [NUM, ID PROGRAMA, NOMBRE DE INICIATIVA, TIPO, BREVE DESCRIPCIÓN, LOCALIDAD, NÚMERO DE LOCALIDAD, UPZ, NÚMERO DE UPZ, UBICACIÓN, COORDENADAS (LATITUD, LONGITUD), CATEGORÍA, ENTIDAD LÍDER, SECTOR, Tramo 1a, Tramo 1b, Tramo 1c, Tramo 1d, Tramo 1e, Tramo 2a, Tramo 2b, Tramo 2c, Tramo 2d, Tramo 2e, Tramo 3a, Tramo 3b, Tramo 3c, Tramo 3d, Tramo 3e, Tramo 3f, Tramo 4a, Tramo 4b, Tramo 4c, Tramo 4d, Tramo 4e, POBLACIÓN, OBSERVACIONES, Unnamed: 37, Unnamed: 38, Unnamed: 39, Unnamed: 40, Unnamed: 41, Unnamed: 42, Unnamed: 43, Unnamed: 44, Unnamed: 45, Unnamed: 46, Unnamed: 47, Unnamed: 48, Unnamed: 49, Unnamed: 50, Unnamed: 51, Unnamed: 52, Unnamed: 53, Unnamed: 54, Unnamed: 55, Unnamed: 56, Unnamed: 57, Unnamed: 58, Unnamed: 59, Unnamed: 60, Unnamed: 61, Unnamed: 62, Unnamed: 63, Unnamed: 64, Unnamed: 65, Unnamed: 66, Unnamed: 67]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 68 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-18b35d782b8b>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['COORDEADAS (Latitud, longitud)'] = coor_series\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Quiere guardar los resultados de la API?(Sí/No)No\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    forma_de_importar = input('¿Importará la base de datos desde el equipo o desde OneDrive?(Equipo/OneDrive)')\n",
    "    if forma_de_importar == 'OneDrive':\n",
    "        site_url = input('Ingresar URL de directorio principal de OneDrive:')\n",
    "        relative_url = input('Ingresar URL del archivo en formato .xlsx a importar de OneDrive:')\n",
    "        user = input('Ingresar nombre de usuario Office')\n",
    "        password = input('Ingresar contraseña')\n",
    "        bytes_file_obj = import_from_Onedrive(site_url, relative_url, user, password)\n",
    "        df = create_df()\n",
    "    elif forma_de_importar == 'Equipo':\n",
    "        bdd_importar = input('Ingresar nombre del archivo en formato .xlsx a importar')\n",
    "        df = create_df()\n",
    "    nuevainfo = input('¿Desea hacer uso de la API o importar archivo del equipo? (API/Equipo)')\n",
    "    if nuevainfo == 'API':\n",
    "        api_key = input('Ingresar contraseña de la API')\n",
    "        coordenadas = get_geoinfo()\n",
    "    elif nuevainfo == 'Equipo':\n",
    "        coordenadas = get_geoinfo()\n",
    "        print(len(coordenadas)) #Conformación de que la longitud de datos geográficos es igual al número de observaciones\n",
    "    coordenadas = emptycorr()\n",
    "    latitudes, longitudes = lat_long()\n",
    "    pointlist, aux_localidades, aux_foundpoints = ref_localidades() \n",
    "    point_dictionary, aux_notfoundpoints = notfound_locpoint()\n",
    "    localidades_name, localidades_num = get_loc_num()\n",
    "    aux_upz, aux_foundupz = ref_upz()\n",
    "    aux_notfoundupz = notfound_upzpoint()\n",
    "    upz_name, upz_num = get_upz_num()\n",
    "    keys, notfountlist, dict_list, result = lostloc_dict()\n",
    "    notfountlistupz, dict_listupz, resultupz = lostupz_dict()\n",
    "    localidades_df, upz_df, final_df = get_finaldf()\n",
    "    save_geocode = input('¿Quiere guardar los resultados de la API?(Sí/No)')\n",
    "    if save_geocode == 'Sí':\n",
    "        save_coor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
